{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @tf.function ：图执行模式\n",
    "https://tf.wiki/zh/basic/tools.html#tf-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然默认的即时执行模式（Eager Execution）为我们带来了灵活及易调试的特性，但在特定的场合，例如追求高性能或部署模型时，我们依然希望使用 TensorFlow 1.X 中默认的图执行模式（Graph Execution），将模型转换为高效的 TensorFlow 图模型。此时，TensorFlow 2 为我们提供了 tf.function 模块，结合 AutoGraph 机制，使得我们仅需加入一个简单的 @tf.function 修饰符，就能轻松将模型以图执行模式运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @tf.function 基础使用方法 \n",
    "在 TensorFlow 2 中，推荐使用 @tf.function （而非 1.X 中的 tf.Session ）实现图执行模式，从而将模型转换为易于部署且高性能的 TensorFlow 图模型。只需要将我们希望以图执行模式运行的代码封装在一个函数内，并在函数前加上 @tf.function 即可，如下例所示。关于 TensorFlow 1.X 版本中的图执行模式可参考 附录 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "#from zh.model.mnist.cnn import CNN\n",
    "#from zh.model.utils import MNISTLoader\n",
    "\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷积层神经元（卷积核）数目\n",
    "            kernel_size=[5, 5],     # 感受野大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3127439\n",
      "loss 2.2556479\n",
      "loss 2.14193726\n",
      "loss 1.96743441\n",
      "loss 1.79970121\n",
      "loss 1.50606477\n",
      "loss 1.37653339\n",
      "loss 1.31806219\n",
      "loss 1.13116229\n",
      "loss 0.851478875\n",
      "loss 0.910327\n",
      "loss 0.961535096\n",
      "loss 0.787063122\n",
      "loss 0.723145902\n",
      "loss 0.499469072\n",
      "loss 0.81531173\n",
      "loss 0.839180827\n",
      "loss 0.741758823\n",
      "loss 0.601519823\n",
      "loss 0.619891644\n",
      "loss 0.532553852\n",
      "loss 0.635587215\n",
      "loss 0.628205061\n",
      "loss 0.762837231\n",
      "loss 0.453942865\n",
      "loss 0.75028044\n",
      "loss 0.354831189\n",
      "loss 0.333435565\n",
      "loss 0.219041973\n",
      "loss 0.335987687\n",
      "loss 0.322120428\n",
      "loss 0.500030816\n",
      "loss 0.241591871\n",
      "loss 0.262259722\n",
      "loss 0.319545656\n",
      "loss 0.251862019\n",
      "loss 0.460616976\n",
      "loss 0.256505877\n",
      "loss 0.450339228\n",
      "loss 0.274512291\n",
      "loss 0.255688816\n",
      "loss 0.246551648\n",
      "loss 0.278954208\n",
      "loss 0.299235731\n",
      "loss 0.152727857\n",
      "loss 0.189101011\n",
      "loss 0.116796158\n",
      "loss 0.122517258\n",
      "loss 0.198961586\n",
      "loss 0.3058002\n",
      "loss 0.171655387\n",
      "loss 0.253458589\n",
      "loss 0.394212902\n",
      "loss 0.347200274\n",
      "loss 0.18580471\n",
      "loss 0.275542945\n",
      "loss 0.179178655\n",
      "loss 0.0748097897\n",
      "loss 0.213784426\n",
      "loss 0.128514245\n",
      "loss 0.0381321684\n",
      "loss 0.0470238701\n",
      "loss 0.190450966\n",
      "loss 0.16305916\n",
      "loss 0.297449529\n",
      "loss 0.277485818\n",
      "loss 0.0558579341\n",
      "loss 0.349193692\n",
      "loss 0.224875599\n",
      "loss 0.107241176\n",
      "loss 0.20112884\n",
      "loss 0.344246566\n",
      "loss 0.0794702768\n",
      "loss 0.174712136\n",
      "loss 0.190446988\n",
      "loss 0.224144861\n",
      "loss 0.226493791\n",
      "loss 0.0507894307\n",
      "loss 0.213861257\n",
      "loss 0.310776323\n",
      "loss 0.211579785\n",
      "loss 0.203322276\n",
      "loss 0.23022747\n",
      "loss 0.198126733\n",
      "loss 0.143882915\n",
      "loss 0.235683903\n",
      "loss 0.0588190556\n",
      "loss 0.223049924\n",
      "loss 0.291924477\n",
      "loss 0.094150871\n",
      "loss 0.339078605\n",
      "loss 0.106032155\n",
      "loss 0.103158362\n",
      "loss 0.168599308\n",
      "loss 0.169922233\n",
      "loss 0.494997323\n",
      "loss 0.146087259\n",
      "loss 0.261775136\n",
      "loss 0.205409124\n",
      "loss 0.235405296\n",
      "loss 0.10436143\n",
      "loss 0.296473891\n",
      "loss 0.355668902\n",
      "loss 0.302089095\n",
      "loss 0.246034026\n",
      "loss 0.233085081\n",
      "loss 0.142252669\n",
      "loss 0.14787358\n",
      "loss 0.144212991\n",
      "loss 0.315259814\n",
      "loss 0.35126698\n",
      "loss 0.149668127\n",
      "loss 0.109123036\n",
      "loss 0.116682522\n",
      "loss 0.189470351\n",
      "loss 0.185824484\n",
      "loss 0.0602942333\n",
      "loss 0.0596440323\n",
      "loss 0.106363297\n",
      "loss 0.094377622\n",
      "loss 0.14092356\n",
      "loss 0.114386871\n",
      "loss 0.123727605\n",
      "loss 0.0640182793\n",
      "loss 0.089352414\n",
      "loss 0.0776306465\n",
      "loss 0.0367569439\n",
      "loss 0.0397226326\n",
      "loss 0.121763192\n",
      "loss 0.0864673257\n",
      "loss 0.188197479\n",
      "loss 0.038844429\n",
      "loss 0.0946764946\n",
      "loss 0.0951850489\n",
      "loss 0.0804707408\n",
      "loss 0.0885129198\n",
      "loss 0.0966779739\n",
      "loss 0.0897044912\n",
      "loss 0.192266956\n",
      "loss 0.264392614\n",
      "loss 0.147364214\n",
      "loss 0.03837559\n",
      "loss 0.0954578295\n",
      "loss 0.0937479138\n",
      "loss 0.072433345\n",
      "loss 0.140293896\n",
      "loss 0.219606966\n",
      "loss 0.163464874\n",
      "loss 0.141529843\n",
      "loss 0.0882853121\n",
      "loss 0.167599812\n",
      "loss 0.119619749\n",
      "loss 0.0415148772\n",
      "loss 0.198210299\n",
      "loss 0.101534888\n",
      "loss 0.0514953621\n",
      "loss 0.203715\n",
      "loss 0.102475472\n",
      "loss 0.194540754\n",
      "loss 0.107567683\n",
      "loss 0.0421607532\n",
      "loss 0.0895154774\n",
      "loss 0.0752678\n",
      "loss 0.105333559\n",
      "loss 0.0945224166\n",
      "loss 0.0345778689\n",
      "loss 0.174955085\n",
      "loss 0.0900739282\n",
      "loss 0.0793594718\n",
      "loss 0.141334265\n",
      "loss 0.0812123939\n",
      "loss 0.0645157\n",
      "loss 0.0474567711\n",
      "loss 0.0232817214\n",
      "loss 0.150083482\n",
      "loss 0.151109189\n",
      "loss 0.0585714728\n",
      "loss 0.202991098\n",
      "loss 0.0745361745\n",
      "loss 0.182529718\n",
      "loss 0.0740602463\n",
      "loss 0.125067562\n",
      "loss 0.0599865578\n",
      "loss 0.0518044196\n",
      "loss 0.134382188\n",
      "loss 0.317243695\n",
      "loss 0.115430944\n",
      "loss 0.0495264418\n",
      "loss 0.04405481\n",
      "loss 0.0656895563\n",
      "loss 0.0649677\n",
      "loss 0.056236811\n",
      "loss 0.105252571\n",
      "loss 0.205804706\n",
      "loss 0.131823137\n",
      "loss 0.0677752271\n",
      "loss 0.156856745\n",
      "loss 0.0356325209\n",
      "loss 0.154625311\n",
      "loss 0.268901438\n",
      "loss 0.0792201906\n",
      "loss 0.0999662876\n",
      "loss 0.144158274\n",
      "loss 0.159036174\n",
      "loss 0.048420582\n",
      "loss 0.0529728048\n",
      "loss 0.089197062\n",
      "loss 0.168249533\n",
      "loss 0.170249671\n",
      "loss 0.0903053284\n",
      "loss 0.0420588553\n",
      "loss 0.0319418907\n",
      "loss 0.119428091\n",
      "loss 0.0868266225\n",
      "loss 0.0550864488\n",
      "loss 0.117422082\n",
      "loss 0.026764879\n",
      "loss 0.0138146095\n",
      "loss 0.153965682\n",
      "loss 0.061066553\n",
      "loss 0.0549964793\n",
      "loss 0.0509198867\n",
      "loss 0.0113572236\n",
      "loss 0.0994610041\n",
      "loss 0.0695980266\n",
      "loss 0.0728847682\n",
      "loss 0.0508412495\n",
      "loss 0.210736707\n",
      "loss 0.0834141\n",
      "loss 0.158502713\n",
      "loss 0.0570267662\n",
      "loss 0.0103541445\n",
      "loss 0.0284986589\n",
      "loss 0.0554991923\n",
      "loss 0.0334451422\n",
      "loss 0.0244781915\n",
      "loss 0.12666896\n",
      "loss 0.256766528\n",
      "loss 0.0295523461\n",
      "loss 0.0718729943\n",
      "loss 0.0811390206\n",
      "loss 0.0307325367\n",
      "loss 0.111375205\n",
      "loss 0.0100398902\n",
      "loss 0.137001023\n",
      "loss 0.0800915137\n",
      "loss 0.0573831499\n",
      "loss 0.05071\n",
      "loss 0.0614398867\n",
      "loss 0.0688719302\n",
      "loss 0.128546089\n",
      "loss 0.00424578832\n",
      "loss 0.212250248\n",
      "loss 0.103664152\n",
      "loss 0.0752022788\n",
      "loss 0.204952762\n",
      "loss 0.0360616595\n",
      "loss 0.155829206\n",
      "loss 0.114601314\n",
      "loss 0.0593813471\n",
      "loss 0.205876708\n",
      "loss 0.116122395\n",
      "loss 0.115924872\n",
      "loss 0.0152840745\n",
      "loss 0.0293971822\n",
      "loss 0.0239806082\n",
      "loss 0.0689363554\n",
      "loss 0.0759523809\n",
      "loss 0.342066258\n",
      "loss 0.0385975838\n",
      "loss 0.0223972145\n",
      "loss 0.192125916\n",
      "loss 0.068342641\n",
      "loss 0.041535344\n",
      "loss 0.165763333\n",
      "loss 0.0537407845\n",
      "loss 0.0963559821\n",
      "loss 0.217841402\n",
      "loss 0.0890100747\n",
      "loss 0.120022491\n",
      "loss 0.0919079557\n",
      "loss 0.0495584756\n",
      "loss 0.0820816159\n",
      "loss 0.109673537\n",
      "loss 0.044797074\n",
      "loss 0.0723546073\n",
      "loss 0.121201232\n",
      "loss 0.0220679864\n",
      "loss 0.0414085276\n",
      "loss 0.0113828098\n",
      "loss 0.205338821\n",
      "loss 0.084453471\n",
      "loss 0.0159788616\n",
      "loss 0.0710459054\n",
      "loss 0.0136499545\n",
      "loss 0.0241549\n",
      "loss 0.0100345062\n",
      "loss 0.0245805699\n",
      "loss 0.0409965515\n",
      "loss 0.0926224515\n",
      "loss 0.049758926\n",
      "loss 0.0163102392\n",
      "loss 0.0650203899\n",
      "loss 0.0456291921\n",
      "loss 0.130227625\n",
      "loss 0.148668468\n",
      "loss 0.00474577257\n",
      "loss 0.298126638\n",
      "loss 0.0229427\n",
      "loss 0.0114260539\n",
      "loss 0.104142763\n",
      "loss 0.0559573211\n",
      "loss 0.0215264503\n",
      "loss 0.180523127\n",
      "loss 0.107020572\n",
      "loss 0.112261467\n",
      "loss 0.0283211228\n",
      "loss 0.263290972\n",
      "loss 0.0320815071\n",
      "loss 0.235556379\n",
      "loss 0.0932298079\n",
      "loss 0.152879551\n",
      "loss 0.118341081\n",
      "loss 0.132169917\n",
      "loss 0.156837121\n",
      "loss 0.0199721884\n",
      "loss 0.0603631288\n",
      "loss 0.11109411\n",
      "loss 0.0979709178\n",
      "loss 0.051345177\n",
      "loss 0.0809356943\n",
      "loss 0.170889944\n",
      "loss 0.0461332239\n",
      "loss 0.0216754824\n",
      "loss 0.0702187046\n",
      "loss 0.0109285275\n",
      "loss 0.0302686337\n",
      "loss 0.210131258\n",
      "loss 0.149146363\n",
      "loss 0.0519606322\n",
      "loss 0.0671333298\n",
      "loss 0.033737693\n",
      "loss 0.00559993647\n",
      "loss 0.1003424\n",
      "loss 0.12031915\n",
      "loss 0.0675047934\n",
      "loss 0.055356849\n",
      "loss 0.0533645712\n",
      "loss 0.00959322136\n",
      "loss 0.15986304\n",
      "loss 0.0630590767\n",
      "loss 0.0797864795\n",
      "loss 0.0758933648\n",
      "loss 0.0171472635\n",
      "loss 0.0150697026\n",
      "loss 0.165347755\n",
      "loss 0.102224067\n",
      "loss 0.0525847971\n",
      "loss 0.200011179\n",
      "loss 0.0546233542\n",
      "loss 0.0986625701\n",
      "loss 0.0919452\n",
      "loss 0.0427978\n",
      "loss 0.0534237809\n",
      "loss 0.0180080961\n",
      "loss 0.128542572\n",
      "loss 0.214911506\n",
      "loss 0.205231115\n",
      "loss 0.0897324756\n",
      "loss 0.118227422\n",
      "loss 0.0120004406\n",
      "loss 0.0230594687\n",
      "loss 0.228613049\n",
      "loss 0.0718998685\n",
      "loss 0.0429253094\n",
      "loss 0.0479441918\n",
      "loss 0.0646362379\n",
      "loss 0.0318364128\n",
      "loss 0.0539521128\n",
      "loss 0.0144331409\n",
      "loss 0.240958616\n",
      "loss 0.0872951597\n",
      "loss 0.102053545\n",
      "loss 0.0082923444\n",
      "loss 0.0645321384\n",
      "loss 0.0441579074\n",
      "loss 0.0414767154\n",
      "loss 0.0500032939\n",
      "loss 0.0141484514\n",
      "loss 0.0749224871\n",
      "loss 0.210711524\n",
      "loss 0.0175958648\n",
      "loss 0.0938487351\n",
      "loss 0.0564479269\n",
      "loss 0.0325023569\n",
      "loss 0.016391011\n",
      "loss 0.15221484\n",
      "loss 0.0487557501\n",
      "loss 0.074839\n",
      "loss 0.150086626\n",
      "2.3813538551330566\n"
     ]
    }
   ],
   "source": [
    "num_batches = 400\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "data_loader = MNISTLoader()\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(X, y):    \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        # 注意这里使用了TensorFlow内置的tf.print()。@tf.function不支持Python内置的print方法\n",
    "        tf.print(\"loss\", loss)  \n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = CNN()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    start_time = time.time()\n",
    "    for batch_index in range(num_batches):\n",
    "        X, y = data_loader.get_batch(batch_size)\n",
    "        train_one_step(X, y)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行 400 个 Batch 进行测试，加入 @tf.function 的程序耗时 35.5 秒，未加入 @tf.function 的纯即时执行模式程序耗时 43.8 秒。可见 @tf.function 带来了一定的性能提升。一般而言，当模型由较多小的操作组成的时候， @tf.function 带来的提升效果较大。而当模型的操作数量较少，但单一操作均很耗时的时候，则 @tf.function 带来的性能提升不会太大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @tf.function 内在机制 \n",
    "当被 @tf.function 修饰的函数第一次被调用的时候，进行以下操作：\n",
    "\n",
    "在即时执行模式关闭的环境下，函数内的代码依次运行。也就是说，每个 tf. 方法都只是定义了计算节点，而并没有进行任何实质的计算。这与 TensorFlow 1.X 的图执行模式是一致的；\n",
    "\n",
    "使用 AutoGraph 将函数中的 Python 控制流语句转换成 TensorFlow 计算图中的对应节点（比如说 while 和 for 语句转换为 tf.while ， if 语句转换为 tf.cond 等等；\n",
    "\n",
    "基于上面的两步，建立函数内代码的计算图表示（为了保证图的计算顺序，图中还会自动加入一些 tf.control_dependencies 节点）；\n",
    "\n",
    "运行一次这个计算图；\n",
    "\n",
    "基于函数的名字和输入的函数参数的类型生成一个哈希值，并将建立的计算图缓存到一个哈希表中。\n",
    "\n",
    "在被 @tf.function 修饰的函数之后再次被调用的时候，根据函数名和输入的函数参数的类型计算哈希值，检查哈希表中是否已经有了对应计算图的缓存。如果是，则直接使用已缓存的计算图，否则重新按上述步骤建立计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一个测试题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function is running in Python\n",
      "1\n",
      "2\n",
      "2\n",
      "The function is running in Python\n",
      "0.1\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    print(\"The function is running in Python\")\n",
    "    tf.print(x)\n",
    "\n",
    "a = tf.constant(1, dtype=tf.int32)\n",
    "f(a)\n",
    "b = tf.constant(2, dtype=tf.int32)\n",
    "f(b)\n",
    "b_ = np.array(2, dtype=np.int32)\n",
    "f(b_)\n",
    "c = tf.constant(0.1, dtype=tf.float32)\n",
    "f(c)\n",
    "d = tf.constant(0.2, dtype=tf.float32)\n",
    "f(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当计算 f(a) 时，由于是第一次调用该函数，TensorFlow 进行了以下操作：\n",
    "\n",
    "将函数内的代码依次运行了一遍（因此输出了文本）；\n",
    "\n",
    "构建了计算图，然后运行了一次该计算图（因此输出了 1）。这里 tf.print(x) 可以作为计算图的节点，但 Python 内置的 print 则不能被转换成计算图的节点。因此，计算图中只包含了 tf.print(x) 这一操作；\n",
    "\n",
    "将该计算图缓存到了一个哈希表中（如果之后再有类型为 tf.int32 ，shape 为空的张量输入，则重复使用已构建的计算图）。\n",
    "\n",
    "计算 f(b) 时，由于 b 的类型与 a 相同，所以 TensorFlow 重复使用了之前已构建的计算图并运行（因此输出了 2）。这里由于并没有真正地逐行运行函数中的代码，所以函数第一行的文本输出代码没有运行。计算 f(b_) 时，TensorFlow 自动将 numpy 的数据结构转换成了 TensorFlow 中的张量，因此依然能够复用之前已构建的计算图。\n",
    "\n",
    "计算 f(c) 时，虽然张量 c 的 shape 和 a 、 b 均相同，但类型为 tf.float32 ，因此 TensorFlow 重新运行了函数内代码（从而再次输出了文本）并建立了一个输入为 tf.float32 类型的计算图。\n",
    "\n",
    "计算 f(d) 时，由于 d 和 c 的类型相同，所以 TensorFlow 复用了计算图，同理没有输出文本。\n",
    "\n",
    "而对于 @tf.function 对 Python 内置的整数和浮点数类型的处理方式，我们通过以下示例展现：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "The function is running in Python\n",
      "1\n",
      "The function is running in Python\n",
      "2\n",
      "1\n",
      "The function is running in Python\n",
      "0.1\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function f at 0x7f6ef6566950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "The function is running in Python\n",
      "0.2\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function f at 0x7f6ef6566950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "f(d)\n",
    "f(1)\n",
    "f(2)\n",
    "f(1)\n",
    "f(0.1)\n",
    "f(0.2)\n",
    "f(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个思考题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.Variable(0.0)\n",
    "\n",
    "@tf.function\n",
    "def g():\n",
    "    a.assign(a + 1.0)\n",
    "    return a\n",
    "\n",
    "print(g())\n",
    "print(g())\n",
    "print(g())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如同正文里的例子一样，你可以在被 @tf.function 修饰的函数里调用 tf.Variable 、 tf.keras.optimizers 、 tf.keras.Model 等包含有变量的数据结构。一旦被调用，这些结构将作为隐含的参数提供给函数。当这些结构内的值在函数内被修改时，在函数外也同样生效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGraph：将 Python 控制流转换为 TensorFlow 计算图 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面提到，@tf.function 使用名为 AutoGraph 的机制将函数中的 Python 控制流语句转换成 TensorFlow 计算图中的对应节点。以下是一个示例，使用 tf.autograph 模块的低层 API tf.autograph.to_code 将函数 square_if_positive 转换成 TensorFlow 计算图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "def tf__square_if_positive(x):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('square_if_positive', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "\n",
      "    def get_state():\n",
      "      return ()\n",
      "\n",
      "    def set_state(_):\n",
      "      pass\n",
      "\n",
      "    def if_true():\n",
      "      x_1, = x,\n",
      "      x_1 = x_1 * x_1\n",
      "      return x_1\n",
      "\n",
      "    def if_false():\n",
      "      x = 0\n",
      "      return x\n",
      "    cond = x > 0\n",
      "    x = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('x',), ())\n",
      "    do_return = True\n",
      "    retval_ = fscope.mark_return_value(x)\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def square_if_positive(x):\n",
    "    if x > 0:\n",
    "        x = x * x\n",
    "    else:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(-1)\n",
    "print(square_if_positive(a), square_if_positive(b))\n",
    "print(tf.autograph.to_code(square_if_positive.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们注意到，原函数中的 Python 控制流 if...else... 被转换为了 x = ag__.if_stmt(cond, if_true, if_false, get_state, set_state) 这种计算图式的写法。AutoGraph 起到了类似编译器的作用，能够帮助我们通过更加自然的 Python 控制流轻松地构建带有条件 / 循环的计算图，而无需手动使用 TensorFlow 的 API 进行构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用传统的 tf.Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，如果你依然钟情于 TensorFlow 传统的图执行模式也没有问题。TensorFlow 2 提供了 tf.compat.v1 模块以支持 TensorFlow 1.X 版本的 API。同时，只要在编写模型的时候稍加注意，Keras 的模型是可以同时兼容即时执行模式和图执行模式的。注意，在图执行模式下， model(input_tensor) 只需运行一次以完成图的建立操作。\n",
    "\n",
    "例如，通过以下代码，同样可以在 MNIST 数据集上训练前面所建立的 MLP 或 CNN 模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于图执行模式的更多内容可参见 [图执行模式下的 TensorFlow](https://tf.wiki/zh/appendix/static.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2condab62182d879ce4f808a6d106065014392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
