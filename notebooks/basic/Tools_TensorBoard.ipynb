{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard：训练过程可视化 \n",
    "https://tf.wiki/zh/basic/tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例：查看多层感知机模型的训练情况 \n",
    "最后提供一个实例，以前章的 多层感知机模型 为例展示 TensorBoard 的使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:15:17.223524Z",
     "start_time": "2020-02-11T11:15:16.202085Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:18:57.002847Z",
     "start_time": "2020-02-11T11:18:49.642712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.440863\n",
      "batch 1: loss 2.351085\n",
      "batch 2: loss 2.354211\n",
      "batch 3: loss 2.186523\n",
      "batch 4: loss 2.132562\n",
      "batch 5: loss 2.077986\n",
      "batch 6: loss 1.987841\n",
      "batch 7: loss 2.004390\n",
      "batch 8: loss 1.803120\n",
      "batch 9: loss 1.756266\n",
      "batch 10: loss 1.730825\n",
      "batch 11: loss 1.706142\n",
      "batch 12: loss 1.720908\n",
      "batch 13: loss 1.661014\n",
      "batch 14: loss 1.520746\n",
      "batch 15: loss 1.509544\n",
      "batch 16: loss 1.407559\n",
      "batch 17: loss 1.490591\n",
      "batch 18: loss 1.297632\n",
      "batch 19: loss 1.172279\n",
      "batch 20: loss 1.151317\n",
      "batch 21: loss 1.183685\n",
      "batch 22: loss 1.186458\n",
      "batch 23: loss 1.155329\n",
      "batch 24: loss 1.314043\n",
      "batch 25: loss 1.078335\n",
      "batch 26: loss 1.080948\n",
      "batch 27: loss 0.929244\n",
      "batch 28: loss 0.936921\n",
      "batch 29: loss 1.106667\n",
      "batch 30: loss 0.782553\n",
      "batch 31: loss 0.997983\n",
      "batch 32: loss 0.935974\n",
      "batch 33: loss 0.998367\n",
      "batch 34: loss 1.074695\n",
      "batch 35: loss 0.871235\n",
      "batch 36: loss 0.990025\n",
      "batch 37: loss 0.799476\n",
      "batch 38: loss 1.011181\n",
      "batch 39: loss 0.895881\n",
      "batch 40: loss 0.652561\n",
      "batch 41: loss 0.894675\n",
      "batch 42: loss 0.616356\n",
      "batch 43: loss 0.754833\n",
      "batch 44: loss 0.625077\n",
      "batch 45: loss 0.757299\n",
      "batch 46: loss 0.604269\n",
      "batch 47: loss 0.738000\n",
      "batch 48: loss 0.588104\n",
      "batch 49: loss 0.422918\n",
      "batch 50: loss 0.663143\n",
      "batch 51: loss 0.498473\n",
      "batch 52: loss 0.622368\n",
      "batch 53: loss 0.417665\n",
      "batch 54: loss 0.567428\n",
      "batch 55: loss 0.672426\n",
      "batch 56: loss 0.611914\n",
      "batch 57: loss 0.720701\n",
      "batch 58: loss 0.653256\n",
      "batch 59: loss 0.530123\n",
      "batch 60: loss 0.428309\n",
      "batch 61: loss 0.623061\n",
      "batch 62: loss 0.405189\n",
      "batch 63: loss 0.678758\n",
      "batch 64: loss 0.705971\n",
      "batch 65: loss 0.444906\n",
      "batch 66: loss 0.407952\n",
      "batch 67: loss 0.453507\n",
      "batch 68: loss 0.530675\n",
      "batch 69: loss 0.670346\n",
      "batch 70: loss 0.548826\n",
      "batch 71: loss 0.377119\n",
      "batch 72: loss 0.610699\n",
      "batch 73: loss 0.598279\n",
      "batch 74: loss 0.513710\n",
      "batch 75: loss 0.475318\n",
      "batch 76: loss 0.460323\n",
      "batch 77: loss 0.572183\n",
      "batch 78: loss 0.621678\n",
      "batch 79: loss 0.701279\n",
      "batch 80: loss 0.563377\n",
      "batch 81: loss 0.361463\n",
      "batch 82: loss 0.442251\n",
      "batch 83: loss 0.638434\n",
      "batch 84: loss 0.444796\n",
      "batch 85: loss 0.478071\n",
      "batch 86: loss 0.473490\n",
      "batch 87: loss 0.449543\n",
      "batch 88: loss 0.610067\n",
      "batch 89: loss 0.398063\n",
      "batch 90: loss 0.313317\n",
      "batch 91: loss 0.332925\n",
      "batch 92: loss 0.402860\n",
      "batch 93: loss 0.365300\n",
      "batch 94: loss 0.470323\n",
      "batch 95: loss 0.413980\n",
      "batch 96: loss 0.476780\n",
      "batch 97: loss 0.495696\n",
      "batch 98: loss 0.401750\n",
      "batch 99: loss 0.357956\n",
      "batch 100: loss 0.505321\n",
      "batch 101: loss 0.461642\n",
      "batch 102: loss 0.609910\n",
      "batch 103: loss 0.328373\n",
      "batch 104: loss 0.279932\n",
      "batch 105: loss 0.439554\n",
      "batch 106: loss 0.536373\n",
      "batch 107: loss 0.419795\n",
      "batch 108: loss 0.475798\n",
      "batch 109: loss 0.377113\n",
      "batch 110: loss 0.512903\n",
      "batch 111: loss 0.350069\n",
      "batch 112: loss 0.423985\n",
      "batch 113: loss 0.513734\n",
      "batch 114: loss 0.237438\n",
      "batch 115: loss 0.476543\n",
      "batch 116: loss 0.398266\n",
      "batch 117: loss 0.350401\n",
      "batch 118: loss 0.306108\n",
      "batch 119: loss 0.249869\n",
      "batch 120: loss 0.349282\n",
      "batch 121: loss 0.398276\n",
      "batch 122: loss 0.320032\n",
      "batch 123: loss 0.308199\n",
      "batch 124: loss 0.448231\n",
      "batch 125: loss 0.384038\n",
      "batch 126: loss 0.343128\n",
      "batch 127: loss 0.592379\n",
      "batch 128: loss 0.393884\n",
      "batch 129: loss 0.320177\n",
      "batch 130: loss 0.487891\n",
      "batch 131: loss 0.443578\n",
      "batch 132: loss 0.456975\n",
      "batch 133: loss 0.312349\n",
      "batch 134: loss 0.408574\n",
      "batch 135: loss 0.440825\n",
      "batch 136: loss 0.301275\n",
      "batch 137: loss 0.490211\n",
      "batch 138: loss 0.573343\n",
      "batch 139: loss 0.469906\n",
      "batch 140: loss 0.224287\n",
      "batch 141: loss 0.262749\n",
      "batch 142: loss 0.568252\n",
      "batch 143: loss 0.446065\n",
      "batch 144: loss 0.354231\n",
      "batch 145: loss 0.570869\n",
      "batch 146: loss 0.479304\n",
      "batch 147: loss 0.531412\n",
      "batch 148: loss 0.204589\n",
      "batch 149: loss 0.368960\n",
      "batch 150: loss 0.514298\n",
      "batch 151: loss 0.201364\n",
      "batch 152: loss 0.334692\n",
      "batch 153: loss 0.439104\n",
      "batch 154: loss 0.380802\n",
      "batch 155: loss 0.271555\n",
      "batch 156: loss 0.377146\n",
      "batch 157: loss 0.380037\n",
      "batch 158: loss 0.216400\n",
      "batch 159: loss 0.226243\n",
      "batch 160: loss 0.379454\n",
      "batch 161: loss 0.339180\n",
      "batch 162: loss 0.197015\n",
      "batch 163: loss 0.274027\n",
      "batch 164: loss 0.377349\n",
      "batch 165: loss 0.326418\n",
      "batch 166: loss 0.304003\n",
      "batch 167: loss 0.414322\n",
      "batch 168: loss 0.581469\n",
      "batch 169: loss 0.345289\n",
      "batch 170: loss 0.288785\n",
      "batch 171: loss 0.463060\n",
      "batch 172: loss 0.317750\n",
      "batch 173: loss 0.342427\n",
      "batch 174: loss 0.408262\n",
      "batch 175: loss 0.375697\n",
      "batch 176: loss 0.491724\n",
      "batch 177: loss 0.253429\n",
      "batch 178: loss 0.292581\n",
      "batch 179: loss 0.333564\n",
      "batch 180: loss 0.503836\n",
      "batch 181: loss 0.289084\n",
      "batch 182: loss 0.362353\n",
      "batch 183: loss 0.285052\n",
      "batch 184: loss 0.646016\n",
      "batch 185: loss 0.349714\n",
      "batch 186: loss 0.225163\n",
      "batch 187: loss 0.432083\n",
      "batch 188: loss 0.249228\n",
      "batch 189: loss 0.421057\n",
      "batch 190: loss 0.240175\n",
      "batch 191: loss 0.263178\n",
      "batch 192: loss 0.441900\n",
      "batch 193: loss 0.511826\n",
      "batch 194: loss 0.581797\n",
      "batch 195: loss 0.181056\n",
      "batch 196: loss 0.313129\n",
      "batch 197: loss 0.399935\n",
      "batch 198: loss 0.292976\n",
      "batch 199: loss 0.245865\n",
      "batch 200: loss 0.522441\n",
      "batch 201: loss 0.294671\n",
      "batch 202: loss 0.366070\n",
      "batch 203: loss 0.467795\n",
      "batch 204: loss 0.348351\n",
      "batch 205: loss 0.358768\n",
      "batch 206: loss 0.244896\n",
      "batch 207: loss 0.397262\n",
      "batch 208: loss 0.667857\n",
      "batch 209: loss 0.322006\n",
      "batch 210: loss 0.588813\n",
      "batch 211: loss 0.355882\n",
      "batch 212: loss 0.259682\n",
      "batch 213: loss 0.388654\n",
      "batch 214: loss 0.234425\n",
      "batch 215: loss 0.223443\n",
      "batch 216: loss 0.277385\n",
      "batch 217: loss 0.703889\n",
      "batch 218: loss 0.463503\n",
      "batch 219: loss 0.512081\n",
      "batch 220: loss 0.295184\n",
      "batch 221: loss 0.174319\n",
      "batch 222: loss 0.347094\n",
      "batch 223: loss 0.367836\n",
      "batch 224: loss 0.281475\n",
      "batch 225: loss 0.426340\n",
      "batch 226: loss 0.475608\n",
      "batch 227: loss 0.399936\n",
      "batch 228: loss 0.158734\n",
      "batch 229: loss 0.217366\n",
      "batch 230: loss 0.623303\n",
      "batch 231: loss 0.324922\n",
      "batch 232: loss 0.371545\n",
      "batch 233: loss 0.371378\n",
      "batch 234: loss 0.355105\n",
      "batch 235: loss 0.440361\n",
      "batch 236: loss 0.407611\n",
      "batch 237: loss 0.309585\n",
      "batch 238: loss 0.400076\n",
      "batch 239: loss 0.299716\n",
      "batch 240: loss 0.369866\n",
      "batch 241: loss 0.202002\n",
      "batch 242: loss 0.270640\n",
      "batch 243: loss 0.277743\n",
      "batch 244: loss 0.407665\n",
      "batch 245: loss 0.332846\n",
      "batch 246: loss 0.231110\n",
      "batch 247: loss 0.451730\n",
      "batch 248: loss 0.406718\n",
      "batch 249: loss 0.185731\n",
      "batch 250: loss 0.257309\n",
      "batch 251: loss 0.587900\n",
      "batch 252: loss 0.302109\n",
      "batch 253: loss 0.502621\n",
      "batch 254: loss 0.417910\n",
      "batch 255: loss 0.421121\n",
      "batch 256: loss 0.191652\n",
      "batch 257: loss 0.297577\n",
      "batch 258: loss 0.254217\n",
      "batch 259: loss 0.159832\n",
      "batch 260: loss 0.299562\n",
      "batch 261: loss 0.193261\n",
      "batch 262: loss 0.184183\n",
      "batch 263: loss 0.391110\n",
      "batch 264: loss 0.159195\n",
      "batch 265: loss 0.315140\n",
      "batch 266: loss 0.530137\n",
      "batch 267: loss 0.223027\n",
      "batch 268: loss 0.233262\n",
      "batch 269: loss 0.264140\n",
      "batch 270: loss 0.320862\n",
      "batch 271: loss 0.507201\n",
      "batch 272: loss 0.173609\n",
      "batch 273: loss 0.216017\n",
      "batch 274: loss 0.379163\n",
      "batch 275: loss 0.375295\n",
      "batch 276: loss 0.362704\n",
      "batch 277: loss 0.168128\n",
      "batch 278: loss 0.482035\n",
      "batch 279: loss 0.415914\n",
      "batch 280: loss 0.212696\n",
      "batch 281: loss 0.183246\n",
      "batch 282: loss 0.464799\n",
      "batch 283: loss 0.306462\n",
      "batch 284: loss 0.504571\n",
      "batch 285: loss 0.363646\n",
      "batch 286: loss 0.498385\n",
      "batch 287: loss 0.538241\n",
      "batch 288: loss 0.238994\n",
      "batch 289: loss 0.293491\n",
      "batch 290: loss 0.446026\n",
      "batch 291: loss 0.328417\n",
      "batch 292: loss 0.201688\n",
      "batch 293: loss 0.562677\n",
      "batch 294: loss 0.231891\n",
      "batch 295: loss 0.397997\n",
      "batch 296: loss 0.158640\n",
      "batch 297: loss 0.187084\n",
      "batch 298: loss 0.523978\n",
      "batch 299: loss 0.257267\n",
      "batch 300: loss 0.234144\n",
      "batch 301: loss 0.310254\n",
      "batch 302: loss 0.176241\n",
      "batch 303: loss 0.279332\n",
      "batch 304: loss 0.197784\n",
      "batch 305: loss 0.307689\n",
      "batch 306: loss 0.271945\n",
      "batch 307: loss 0.183888\n",
      "batch 308: loss 0.344253\n",
      "batch 309: loss 0.358883\n",
      "batch 310: loss 0.177818\n",
      "batch 311: loss 0.265907\n",
      "batch 312: loss 0.231958\n",
      "batch 313: loss 0.213554\n",
      "batch 314: loss 0.274268\n",
      "batch 315: loss 0.175473\n",
      "batch 316: loss 0.158095\n",
      "batch 317: loss 0.274782\n",
      "batch 318: loss 0.317684\n",
      "batch 319: loss 0.232922\n",
      "batch 320: loss 0.266508\n",
      "batch 321: loss 0.228853\n",
      "batch 322: loss 0.310533\n",
      "batch 323: loss 0.256037\n",
      "batch 324: loss 0.252554\n",
      "batch 325: loss 0.436704\n",
      "batch 326: loss 0.359512\n",
      "batch 327: loss 0.439629\n",
      "batch 328: loss 0.167555\n",
      "batch 329: loss 0.242008\n",
      "batch 330: loss 0.325299\n",
      "batch 331: loss 0.244409\n",
      "batch 332: loss 0.224036\n",
      "batch 333: loss 0.454476\n",
      "batch 334: loss 0.422967\n",
      "batch 335: loss 0.312086\n",
      "batch 336: loss 0.373075\n",
      "batch 337: loss 0.210415\n",
      "batch 338: loss 0.257333\n",
      "batch 339: loss 0.385113\n",
      "batch 340: loss 0.474441\n",
      "batch 341: loss 0.254247\n",
      "batch 342: loss 0.248496\n",
      "batch 343: loss 0.180065\n",
      "batch 344: loss 0.435861\n",
      "batch 345: loss 0.193658\n",
      "batch 346: loss 0.311395\n",
      "batch 347: loss 0.154784\n",
      "batch 348: loss 0.235881\n",
      "batch 349: loss 0.234024\n",
      "batch 350: loss 0.176108\n",
      "batch 351: loss 0.155390\n",
      "batch 352: loss 0.257854\n",
      "batch 353: loss 0.198678\n",
      "batch 354: loss 0.198003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 355: loss 0.379764\n",
      "batch 356: loss 0.395259\n",
      "batch 357: loss 0.187741\n",
      "batch 358: loss 0.163589\n",
      "batch 359: loss 0.276098\n",
      "batch 360: loss 0.289661\n",
      "batch 361: loss 0.257722\n",
      "batch 362: loss 0.230889\n",
      "batch 363: loss 0.313603\n",
      "batch 364: loss 0.374112\n",
      "batch 365: loss 0.389441\n",
      "batch 366: loss 0.198932\n",
      "batch 367: loss 0.330717\n",
      "batch 368: loss 0.170879\n",
      "batch 369: loss 0.275873\n",
      "batch 370: loss 0.221428\n",
      "batch 371: loss 0.193662\n",
      "batch 372: loss 0.309786\n",
      "batch 373: loss 0.519774\n",
      "batch 374: loss 0.258954\n",
      "batch 375: loss 0.188091\n",
      "batch 376: loss 0.501827\n",
      "batch 377: loss 0.377193\n",
      "batch 378: loss 0.225625\n",
      "batch 379: loss 0.170722\n",
      "batch 380: loss 0.213849\n",
      "batch 381: loss 0.335915\n",
      "batch 382: loss 0.204249\n",
      "batch 383: loss 0.225048\n",
      "batch 384: loss 0.209855\n",
      "batch 385: loss 0.529824\n",
      "batch 386: loss 0.266728\n",
      "batch 387: loss 0.127914\n",
      "batch 388: loss 0.264822\n",
      "batch 389: loss 0.251116\n",
      "batch 390: loss 0.336221\n",
      "batch 391: loss 0.162031\n",
      "batch 392: loss 0.174867\n",
      "batch 393: loss 0.127399\n",
      "batch 394: loss 0.394436\n",
      "batch 395: loss 0.378832\n",
      "batch 396: loss 0.295223\n",
      "batch 397: loss 0.310650\n",
      "batch 398: loss 0.303581\n",
      "batch 399: loss 0.331828\n",
      "batch 400: loss 0.652053\n",
      "batch 401: loss 0.660341\n",
      "batch 402: loss 0.423295\n",
      "batch 403: loss 0.158063\n",
      "batch 404: loss 0.104487\n",
      "batch 405: loss 0.204105\n",
      "batch 406: loss 0.387858\n",
      "batch 407: loss 0.250026\n",
      "batch 408: loss 0.396770\n",
      "batch 409: loss 0.180280\n",
      "batch 410: loss 0.323004\n",
      "batch 411: loss 0.148671\n",
      "batch 412: loss 0.430995\n",
      "batch 413: loss 0.165289\n",
      "batch 414: loss 0.159584\n",
      "batch 415: loss 0.191818\n",
      "batch 416: loss 0.373419\n",
      "batch 417: loss 0.109572\n",
      "batch 418: loss 0.233113\n",
      "batch 419: loss 0.444480\n",
      "batch 420: loss 0.184640\n",
      "batch 421: loss 0.208515\n",
      "batch 422: loss 0.298535\n",
      "batch 423: loss 0.310686\n",
      "batch 424: loss 0.402633\n",
      "batch 425: loss 0.221216\n",
      "batch 426: loss 0.173872\n",
      "batch 427: loss 0.306165\n",
      "batch 428: loss 0.261351\n",
      "batch 429: loss 0.188239\n",
      "batch 430: loss 0.128371\n",
      "batch 431: loss 0.235737\n",
      "batch 432: loss 0.308262\n",
      "batch 433: loss 0.383812\n",
      "batch 434: loss 0.368701\n",
      "batch 435: loss 0.282924\n",
      "batch 436: loss 0.381007\n",
      "batch 437: loss 0.271148\n",
      "batch 438: loss 0.342220\n",
      "batch 439: loss 0.473008\n",
      "batch 440: loss 0.208267\n",
      "batch 441: loss 0.251295\n",
      "batch 442: loss 0.253918\n",
      "batch 443: loss 0.261044\n",
      "batch 444: loss 0.131182\n",
      "batch 445: loss 0.369408\n",
      "batch 446: loss 0.244946\n",
      "batch 447: loss 0.497428\n",
      "batch 448: loss 0.714604\n",
      "batch 449: loss 0.230928\n",
      "batch 450: loss 0.377576\n",
      "batch 451: loss 0.263201\n",
      "batch 452: loss 0.267841\n",
      "batch 453: loss 0.158599\n",
      "batch 454: loss 0.191120\n",
      "batch 455: loss 0.324199\n",
      "batch 456: loss 0.298036\n",
      "batch 457: loss 0.267795\n",
      "batch 458: loss 0.371294\n",
      "batch 459: loss 0.244739\n",
      "batch 460: loss 0.304331\n",
      "batch 461: loss 0.315956\n",
      "batch 462: loss 0.124069\n",
      "batch 463: loss 0.223837\n",
      "batch 464: loss 0.259587\n",
      "batch 465: loss 0.152218\n",
      "batch 466: loss 0.334240\n",
      "batch 467: loss 0.277972\n",
      "batch 468: loss 0.274597\n",
      "batch 469: loss 0.139974\n",
      "batch 470: loss 0.191748\n",
      "batch 471: loss 0.294472\n",
      "batch 472: loss 0.220934\n",
      "batch 473: loss 0.191565\n",
      "batch 474: loss 0.176560\n",
      "batch 475: loss 0.263022\n",
      "batch 476: loss 0.066162\n",
      "batch 477: loss 0.365869\n",
      "batch 478: loss 0.137210\n",
      "batch 479: loss 0.126191\n",
      "batch 480: loss 0.487605\n",
      "batch 481: loss 0.122372\n",
      "batch 482: loss 0.190833\n",
      "batch 483: loss 0.355691\n",
      "batch 484: loss 0.320608\n",
      "batch 485: loss 0.239402\n",
      "batch 486: loss 0.086336\n",
      "batch 487: loss 0.207613\n",
      "batch 488: loss 0.354815\n",
      "batch 489: loss 0.437886\n",
      "batch 490: loss 0.373387\n",
      "batch 491: loss 0.218906\n",
      "batch 492: loss 0.347131\n",
      "batch 493: loss 0.366068\n",
      "batch 494: loss 0.216062\n",
      "batch 495: loss 0.419496\n",
      "batch 496: loss 0.366730\n",
      "batch 497: loss 0.301704\n",
      "batch 498: loss 0.230946\n",
      "batch 499: loss 0.267266\n",
      "batch 500: loss 0.266764\n",
      "batch 501: loss 0.159601\n",
      "batch 502: loss 0.247352\n",
      "batch 503: loss 0.188842\n",
      "batch 504: loss 0.301125\n",
      "batch 505: loss 0.136188\n",
      "batch 506: loss 0.408374\n",
      "batch 507: loss 0.148100\n",
      "batch 508: loss 0.239288\n",
      "batch 509: loss 0.165793\n",
      "batch 510: loss 0.185893\n",
      "batch 511: loss 0.127215\n",
      "batch 512: loss 0.232123\n",
      "batch 513: loss 0.230150\n",
      "batch 514: loss 0.262405\n",
      "batch 515: loss 0.357688\n",
      "batch 516: loss 0.414604\n",
      "batch 517: loss 0.200183\n",
      "batch 518: loss 0.155135\n",
      "batch 519: loss 0.258450\n",
      "batch 520: loss 0.355600\n",
      "batch 521: loss 0.524273\n",
      "batch 522: loss 0.379358\n",
      "batch 523: loss 0.195593\n",
      "batch 524: loss 0.414720\n",
      "batch 525: loss 0.099397\n",
      "batch 526: loss 0.256014\n",
      "batch 527: loss 0.232239\n",
      "batch 528: loss 0.132601\n",
      "batch 529: loss 0.150891\n",
      "batch 530: loss 0.172266\n",
      "batch 531: loss 0.308676\n",
      "batch 532: loss 0.331301\n",
      "batch 533: loss 0.270651\n",
      "batch 534: loss 0.140537\n",
      "batch 535: loss 0.301749\n",
      "batch 536: loss 0.158104\n",
      "batch 537: loss 0.177478\n",
      "batch 538: loss 0.291687\n",
      "batch 539: loss 0.119487\n",
      "batch 540: loss 0.304576\n",
      "batch 541: loss 0.272410\n",
      "batch 542: loss 0.163346\n",
      "batch 543: loss 0.420031\n",
      "batch 544: loss 0.344028\n",
      "batch 545: loss 0.225818\n",
      "batch 546: loss 0.431653\n",
      "batch 547: loss 0.228769\n",
      "batch 548: loss 0.216454\n",
      "batch 549: loss 0.301938\n",
      "batch 550: loss 0.291901\n",
      "batch 551: loss 0.184107\n",
      "batch 552: loss 0.294722\n",
      "batch 553: loss 0.370585\n",
      "batch 554: loss 0.345413\n",
      "batch 555: loss 0.295759\n",
      "batch 556: loss 0.246682\n",
      "batch 557: loss 0.211053\n",
      "batch 558: loss 0.281612\n",
      "batch 559: loss 0.320346\n",
      "batch 560: loss 0.062343\n",
      "batch 561: loss 0.213740\n",
      "batch 562: loss 0.307046\n",
      "batch 563: loss 0.458899\n",
      "batch 564: loss 0.129778\n",
      "batch 565: loss 0.262041\n",
      "batch 566: loss 0.067668\n",
      "batch 567: loss 0.213430\n",
      "batch 568: loss 0.290452\n",
      "batch 569: loss 0.333306\n",
      "batch 570: loss 0.340193\n",
      "batch 571: loss 0.099685\n",
      "batch 572: loss 0.179612\n",
      "batch 573: loss 0.206695\n",
      "batch 574: loss 0.431484\n",
      "batch 575: loss 0.250876\n",
      "batch 576: loss 0.300869\n",
      "batch 577: loss 0.205578\n",
      "batch 578: loss 0.216494\n",
      "batch 579: loss 0.255938\n",
      "batch 580: loss 0.277376\n",
      "batch 581: loss 0.229964\n",
      "batch 582: loss 0.206877\n",
      "batch 583: loss 0.295358\n",
      "batch 584: loss 0.128485\n",
      "batch 585: loss 0.229318\n",
      "batch 586: loss 0.166929\n",
      "batch 587: loss 0.239292\n",
      "batch 588: loss 0.165470\n",
      "batch 589: loss 0.185092\n",
      "batch 590: loss 0.285110\n",
      "batch 591: loss 0.256157\n",
      "batch 592: loss 0.302700\n",
      "batch 593: loss 0.213116\n",
      "batch 594: loss 0.253586\n",
      "batch 595: loss 0.228890\n",
      "batch 596: loss 0.287635\n",
      "batch 597: loss 0.200113\n",
      "batch 598: loss 0.202778\n",
      "batch 599: loss 0.168043\n",
      "batch 600: loss 0.179985\n",
      "batch 601: loss 0.208070\n",
      "batch 602: loss 0.078565\n",
      "batch 603: loss 0.415253\n",
      "batch 604: loss 0.396702\n",
      "batch 605: loss 0.262388\n",
      "batch 606: loss 0.108928\n",
      "batch 607: loss 0.367579\n",
      "batch 608: loss 0.219673\n",
      "batch 609: loss 0.284666\n",
      "batch 610: loss 0.547464\n",
      "batch 611: loss 0.113394\n",
      "batch 612: loss 0.274505\n",
      "batch 613: loss 0.327832\n",
      "batch 614: loss 0.318192\n",
      "batch 615: loss 0.201435\n",
      "batch 616: loss 0.158177\n",
      "batch 617: loss 0.093881\n",
      "batch 618: loss 0.372875\n",
      "batch 619: loss 0.454282\n",
      "batch 620: loss 0.216789\n",
      "batch 621: loss 0.519818\n",
      "batch 622: loss 0.098733\n",
      "batch 623: loss 0.218018\n",
      "batch 624: loss 0.186898\n",
      "batch 625: loss 0.222646\n",
      "batch 626: loss 0.351366\n",
      "batch 627: loss 0.171189\n",
      "batch 628: loss 0.429626\n",
      "batch 629: loss 0.327308\n",
      "batch 630: loss 0.188237\n",
      "batch 631: loss 0.325794\n",
      "batch 632: loss 0.341397\n",
      "batch 633: loss 0.394356\n",
      "batch 634: loss 0.177994\n",
      "batch 635: loss 0.161411\n",
      "batch 636: loss 0.439281\n",
      "batch 637: loss 0.188823\n",
      "batch 638: loss 0.158167\n",
      "batch 639: loss 0.202336\n",
      "batch 640: loss 0.210111\n",
      "batch 641: loss 0.199293\n",
      "batch 642: loss 0.202243\n",
      "batch 643: loss 0.210946\n",
      "batch 644: loss 0.297213\n",
      "batch 645: loss 0.189985\n",
      "batch 646: loss 0.117706\n",
      "batch 647: loss 0.203798\n",
      "batch 648: loss 0.130052\n",
      "batch 649: loss 0.078308\n",
      "batch 650: loss 0.308478\n",
      "batch 651: loss 0.418685\n",
      "batch 652: loss 0.301892\n",
      "batch 653: loss 0.219272\n",
      "batch 654: loss 0.207144\n",
      "batch 655: loss 0.318066\n",
      "batch 656: loss 0.476128\n",
      "batch 657: loss 0.346873\n",
      "batch 658: loss 0.196816\n",
      "batch 659: loss 0.207345\n",
      "batch 660: loss 0.562206\n",
      "batch 661: loss 0.181252\n",
      "batch 662: loss 0.214568\n",
      "batch 663: loss 0.234733\n",
      "batch 664: loss 0.218446\n",
      "batch 665: loss 0.297145\n",
      "batch 666: loss 0.133604\n",
      "batch 667: loss 0.425109\n",
      "batch 668: loss 0.196296\n",
      "batch 669: loss 0.321798\n",
      "batch 670: loss 0.220894\n",
      "batch 671: loss 0.174203\n",
      "batch 672: loss 0.278649\n",
      "batch 673: loss 0.217488\n",
      "batch 674: loss 0.318365\n",
      "batch 675: loss 0.128412\n",
      "batch 676: loss 0.230606\n",
      "batch 677: loss 0.352439\n",
      "batch 678: loss 0.179001\n",
      "batch 679: loss 0.229624\n",
      "batch 680: loss 0.235692\n",
      "batch 681: loss 0.145303\n",
      "batch 682: loss 0.206388\n",
      "batch 683: loss 0.207623\n",
      "batch 684: loss 0.283652\n",
      "batch 685: loss 0.329713\n",
      "batch 686: loss 0.213580\n",
      "batch 687: loss 0.118707\n",
      "batch 688: loss 0.111601\n",
      "batch 689: loss 0.136378\n",
      "batch 690: loss 0.198128\n",
      "batch 691: loss 0.265485\n",
      "batch 692: loss 0.333153\n",
      "batch 693: loss 0.286613\n",
      "batch 694: loss 0.110635\n",
      "batch 695: loss 0.146711\n",
      "batch 696: loss 0.329661\n",
      "batch 697: loss 0.120758\n",
      "batch 698: loss 0.191832\n",
      "batch 699: loss 0.105722\n",
      "batch 700: loss 0.112658\n",
      "batch 701: loss 0.174704\n",
      "batch 702: loss 0.177132\n",
      "batch 703: loss 0.319612\n",
      "batch 704: loss 0.111923\n",
      "batch 705: loss 0.239549\n",
      "batch 706: loss 0.302692\n",
      "batch 707: loss 0.185355\n",
      "batch 708: loss 0.129009\n",
      "batch 709: loss 0.350445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 710: loss 0.264258\n",
      "batch 711: loss 0.103257\n",
      "batch 712: loss 0.226038\n",
      "batch 713: loss 0.201210\n",
      "batch 714: loss 0.197025\n",
      "batch 715: loss 0.249662\n",
      "batch 716: loss 0.341942\n",
      "batch 717: loss 0.168882\n",
      "batch 718: loss 0.265138\n",
      "batch 719: loss 0.189167\n",
      "batch 720: loss 0.158369\n",
      "batch 721: loss 0.306120\n",
      "batch 722: loss 0.176407\n",
      "batch 723: loss 0.246556\n",
      "batch 724: loss 0.421845\n",
      "batch 725: loss 0.306097\n",
      "batch 726: loss 0.331393\n",
      "batch 727: loss 0.229816\n",
      "batch 728: loss 0.250017\n",
      "batch 729: loss 0.146112\n",
      "batch 730: loss 0.106469\n",
      "batch 731: loss 0.229679\n",
      "batch 732: loss 0.204585\n",
      "batch 733: loss 0.276972\n",
      "batch 734: loss 0.182763\n",
      "batch 735: loss 0.263279\n",
      "batch 736: loss 0.208866\n",
      "batch 737: loss 0.159641\n",
      "batch 738: loss 0.166756\n",
      "batch 739: loss 0.075537\n",
      "batch 740: loss 0.073339\n",
      "batch 741: loss 0.101506\n",
      "batch 742: loss 0.275496\n",
      "batch 743: loss 0.232008\n",
      "batch 744: loss 0.157162\n",
      "batch 745: loss 0.095400\n",
      "batch 746: loss 0.114575\n",
      "batch 747: loss 0.407539\n",
      "batch 748: loss 0.126552\n",
      "batch 749: loss 0.147467\n",
      "batch 750: loss 0.111390\n",
      "batch 751: loss 0.241332\n",
      "batch 752: loss 0.289231\n",
      "batch 753: loss 0.248552\n",
      "batch 754: loss 0.354403\n",
      "batch 755: loss 0.261976\n",
      "batch 756: loss 0.181271\n",
      "batch 757: loss 0.306901\n",
      "batch 758: loss 0.178955\n",
      "batch 759: loss 0.239928\n",
      "batch 760: loss 0.204626\n",
      "batch 761: loss 0.321286\n",
      "batch 762: loss 0.209375\n",
      "batch 763: loss 0.272503\n",
      "batch 764: loss 0.256435\n",
      "batch 765: loss 0.082128\n",
      "batch 766: loss 0.206941\n",
      "batch 767: loss 0.161751\n",
      "batch 768: loss 0.157636\n",
      "batch 769: loss 0.326261\n",
      "batch 770: loss 0.126147\n",
      "batch 771: loss 0.074921\n",
      "batch 772: loss 0.120351\n",
      "batch 773: loss 0.188742\n",
      "batch 774: loss 0.130771\n",
      "batch 775: loss 0.232287\n",
      "batch 776: loss 0.295432\n",
      "batch 777: loss 0.126126\n",
      "batch 778: loss 0.048452\n",
      "batch 779: loss 0.348270\n",
      "batch 780: loss 0.198657\n",
      "batch 781: loss 0.167173\n",
      "batch 782: loss 0.189072\n",
      "batch 783: loss 0.239076\n",
      "batch 784: loss 0.115346\n",
      "batch 785: loss 0.278102\n",
      "batch 786: loss 0.199448\n",
      "batch 787: loss 0.283665\n",
      "batch 788: loss 0.266829\n",
      "batch 789: loss 0.205129\n",
      "batch 790: loss 0.218070\n",
      "batch 791: loss 0.121811\n",
      "batch 792: loss 0.055185\n",
      "batch 793: loss 0.287529\n",
      "batch 794: loss 0.059577\n",
      "batch 795: loss 0.306592\n",
      "batch 796: loss 0.387199\n",
      "batch 797: loss 0.080616\n",
      "batch 798: loss 0.429798\n",
      "batch 799: loss 0.386686\n",
      "batch 800: loss 0.070156\n",
      "batch 801: loss 0.125990\n",
      "batch 802: loss 0.213786\n",
      "batch 803: loss 0.147001\n",
      "batch 804: loss 0.106289\n",
      "batch 805: loss 0.236380\n",
      "batch 806: loss 0.202372\n",
      "batch 807: loss 0.266753\n",
      "batch 808: loss 0.180713\n",
      "batch 809: loss 0.180833\n",
      "batch 810: loss 0.225513\n",
      "batch 811: loss 0.376229\n",
      "batch 812: loss 0.200425\n",
      "batch 813: loss 0.199898\n",
      "batch 814: loss 0.188163\n",
      "batch 815: loss 0.277679\n",
      "batch 816: loss 0.149016\n",
      "batch 817: loss 0.153670\n",
      "batch 818: loss 0.728865\n",
      "batch 819: loss 0.471049\n",
      "batch 820: loss 0.210339\n",
      "batch 821: loss 0.161090\n",
      "batch 822: loss 0.127889\n",
      "batch 823: loss 0.268822\n",
      "batch 824: loss 0.393602\n",
      "batch 825: loss 0.211405\n",
      "batch 826: loss 0.074432\n",
      "batch 827: loss 0.176568\n",
      "batch 828: loss 0.089850\n",
      "batch 829: loss 0.394024\n",
      "batch 830: loss 0.291461\n",
      "batch 831: loss 0.205554\n",
      "batch 832: loss 0.209501\n",
      "batch 833: loss 0.127974\n",
      "batch 834: loss 0.097507\n",
      "batch 835: loss 0.143222\n",
      "batch 836: loss 0.242293\n",
      "batch 837: loss 0.136971\n",
      "batch 838: loss 0.168911\n",
      "batch 839: loss 0.101772\n",
      "batch 840: loss 0.098465\n",
      "batch 841: loss 0.130302\n",
      "batch 842: loss 0.265777\n",
      "batch 843: loss 0.158050\n",
      "batch 844: loss 0.215053\n",
      "batch 845: loss 0.344991\n",
      "batch 846: loss 0.245327\n",
      "batch 847: loss 0.276055\n",
      "batch 848: loss 0.247568\n",
      "batch 849: loss 0.162610\n",
      "batch 850: loss 0.197099\n",
      "batch 851: loss 0.208413\n",
      "batch 852: loss 0.202671\n",
      "batch 853: loss 0.298709\n",
      "batch 854: loss 0.262242\n",
      "batch 855: loss 0.530270\n",
      "batch 856: loss 0.188727\n",
      "batch 857: loss 0.210735\n",
      "batch 858: loss 0.196470\n",
      "batch 859: loss 0.327725\n",
      "batch 860: loss 0.208998\n",
      "batch 861: loss 0.223535\n",
      "batch 862: loss 0.306244\n",
      "batch 863: loss 0.400780\n",
      "batch 864: loss 0.159762\n",
      "batch 865: loss 0.244237\n",
      "batch 866: loss 0.161494\n",
      "batch 867: loss 0.308189\n",
      "batch 868: loss 0.205639\n",
      "batch 869: loss 0.179728\n",
      "batch 870: loss 0.157028\n",
      "batch 871: loss 0.266245\n",
      "batch 872: loss 0.151109\n",
      "batch 873: loss 0.480351\n",
      "batch 874: loss 0.107563\n",
      "batch 875: loss 0.109189\n",
      "batch 876: loss 0.145163\n",
      "batch 877: loss 0.111801\n",
      "batch 878: loss 0.147713\n",
      "batch 879: loss 0.272669\n",
      "batch 880: loss 0.238724\n",
      "batch 881: loss 0.280575\n",
      "batch 882: loss 0.306307\n",
      "batch 883: loss 0.132896\n",
      "batch 884: loss 0.295853\n",
      "batch 885: loss 0.203026\n",
      "batch 886: loss 0.220132\n",
      "batch 887: loss 0.129353\n",
      "batch 888: loss 0.295546\n",
      "batch 889: loss 0.140585\n",
      "batch 890: loss 0.483480\n",
      "batch 891: loss 0.224959\n",
      "batch 892: loss 0.147661\n",
      "batch 893: loss 0.175036\n",
      "batch 894: loss 0.094304\n",
      "batch 895: loss 0.352621\n",
      "batch 896: loss 0.245551\n",
      "batch 897: loss 0.162689\n",
      "batch 898: loss 0.429223\n",
      "batch 899: loss 0.189539\n",
      "batch 900: loss 0.261389\n",
      "batch 901: loss 0.172660\n",
      "batch 902: loss 0.099641\n",
      "batch 903: loss 0.187619\n",
      "batch 904: loss 0.296330\n",
      "batch 905: loss 0.197603\n",
      "batch 906: loss 0.330307\n",
      "batch 907: loss 0.188272\n",
      "batch 908: loss 0.263101\n",
      "batch 909: loss 0.200304\n",
      "batch 910: loss 0.354160\n",
      "batch 911: loss 0.113634\n",
      "batch 912: loss 0.104548\n",
      "batch 913: loss 0.143165\n",
      "batch 914: loss 0.715729\n",
      "batch 915: loss 0.142164\n",
      "batch 916: loss 0.244524\n",
      "batch 917: loss 0.163705\n",
      "batch 918: loss 0.209200\n",
      "batch 919: loss 0.174056\n",
      "batch 920: loss 0.193526\n",
      "batch 921: loss 0.094240\n",
      "batch 922: loss 0.190898\n",
      "batch 923: loss 0.096402\n",
      "batch 924: loss 0.144597\n",
      "batch 925: loss 0.161094\n",
      "batch 926: loss 0.093778\n",
      "batch 927: loss 0.071114\n",
      "batch 928: loss 0.238577\n",
      "batch 929: loss 0.237105\n",
      "batch 930: loss 0.166826\n",
      "batch 931: loss 0.077119\n",
      "batch 932: loss 0.320571\n",
      "batch 933: loss 0.142776\n",
      "batch 934: loss 0.391046\n",
      "batch 935: loss 0.110762\n",
      "batch 936: loss 0.253715\n",
      "batch 937: loss 0.101435\n",
      "batch 938: loss 0.187839\n",
      "batch 939: loss 0.116968\n",
      "batch 940: loss 0.188214\n",
      "batch 941: loss 0.180516\n",
      "batch 942: loss 0.227437\n",
      "batch 943: loss 0.129910\n",
      "batch 944: loss 0.388656\n",
      "batch 945: loss 0.132495\n",
      "batch 946: loss 0.131287\n",
      "batch 947: loss 0.227314\n",
      "batch 948: loss 0.282994\n",
      "batch 949: loss 0.366121\n",
      "batch 950: loss 0.161794\n",
      "batch 951: loss 0.049017\n",
      "batch 952: loss 0.174447\n",
      "batch 953: loss 0.156793\n",
      "batch 954: loss 0.121288\n",
      "batch 955: loss 0.346071\n",
      "batch 956: loss 0.234465\n",
      "batch 957: loss 0.071995\n",
      "batch 958: loss 0.258067\n",
      "batch 959: loss 0.084290\n",
      "batch 960: loss 0.263318\n",
      "batch 961: loss 0.092118\n",
      "batch 962: loss 0.083750\n",
      "batch 963: loss 0.177766\n",
      "batch 964: loss 0.323876\n",
      "batch 965: loss 0.222700\n",
      "batch 966: loss 0.149464\n",
      "batch 967: loss 0.138185\n",
      "batch 968: loss 0.139929\n",
      "batch 969: loss 0.174409\n",
      "batch 970: loss 0.164173\n",
      "batch 971: loss 0.222475\n",
      "batch 972: loss 0.203166\n",
      "batch 973: loss 0.175973\n",
      "batch 974: loss 0.069140\n",
      "batch 975: loss 0.140543\n",
      "batch 976: loss 0.359368\n",
      "batch 977: loss 0.163628\n",
      "batch 978: loss 0.207388\n",
      "batch 979: loss 0.197204\n",
      "batch 980: loss 0.114808\n",
      "batch 981: loss 0.276488\n",
      "batch 982: loss 0.242405\n",
      "batch 983: loss 0.277002\n",
      "batch 984: loss 0.161485\n",
      "batch 985: loss 0.148368\n",
      "batch 986: loss 0.076354\n",
      "batch 987: loss 0.170116\n",
      "batch 988: loss 0.091974\n",
      "batch 989: loss 0.088562\n",
      "batch 990: loss 0.149422\n",
      "batch 991: loss 0.185591\n",
      "batch 992: loss 0.158153\n",
      "batch 993: loss 0.051649\n",
      "batch 994: loss 0.163427\n",
      "batch 995: loss 0.105691\n",
      "batch 996: loss 0.169956\n",
      "batch 997: loss 0.319707\n",
      "batch 998: loss 0.141505\n",
      "batch 999: loss 0.203066\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "log_dir = 'tensorboard'\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)     # 实例化记录器\n",
    "tf.summary.trace_on(profiler=True)  # 开启Trace（可选）\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        with summary_writer.as_default():                           # 指定记录器\n",
    "            tf.summary.scalar(\"loss\", loss, step=batch_index)       # 将当前损失函数的值写入记录器\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=log_dir)    # 保存Trace信息到文件（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
