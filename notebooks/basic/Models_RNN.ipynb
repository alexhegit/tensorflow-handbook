{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 模型建立与训练\n",
    "https://tf.wiki/zh/basic/models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络（RNN）\n",
    "https://tf.wiki/zh/basic/models.html#rnn\n",
    "\n",
    "循环神经网络（Recurrent Neural Network, RNN）是一种适宜于处理序列数据的神经网络，被广泛用于语言模型、文本生成、机器翻译等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，还是实现一个简单的 DataLoader 类来读取文本，并以字符为单位进行编码。设字符种类数为 num_chars ，则每种字符赋予一个 0 到 num_chars - 1 之间的唯一整数编号 i。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)\n",
    "\n",
    "    def predict(self, inputs, temperature=1.):\n",
    "        batch_size, _ = tf.shape(inputs)\n",
    "        logits = self(inputs, from_logits=True)\n",
    "        prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "        return np.array([np.random.choice(self.num_chars, p=prob[i, :])\n",
    "                         for i in range(batch_size.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99: loss 2.985571\n",
      "batch 199: loss 2.846085\n",
      "batch 299: loss 2.986976\n",
      "batch 399: loss 2.905682\n",
      "batch 499: loss 2.350620\n",
      "batch 599: loss 2.478311\n",
      "batch 699: loss 2.398890\n",
      "batch 799: loss 2.610779\n",
      "batch 899: loss 2.781663\n",
      "batch 999: loss 2.536434\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if (batch_index + 1) % 100 == 0:\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity 0.200000:\n",
      "ere the the the the the he the the the the the the the the the the the the the and and of the the the the the ind of mathe the the the the the the the the the at ous of the and ous of of the the and of the the and of the the he the and of the the se the the andere the fere the the the the the the the the seese the the the the the the the the and ond on the the the the the the the the he the the th\n",
      "\n",
      "diversity 0.500000:\n",
      "e ere what\n",
      "of arecere  on the the thas and or the the alithen the wion tithe\n",
      "ferout oust of whe the the mouthe he the his so the there more lution the pite deriom the nouss ousas the here the aude tous outhers and\n",
      "marte the the caeder of the ande- and ind of of the we thit outinty the siche\n",
      "er ind ou te the ousiol of inpe aon mothoutipertere toa the ceateeres ot oo the aude the the pereos the worl\n",
      "\n",
      "diversity 1.000000:\n",
      "ese pedethaa\n",
      "e bimman- atdony\n",
      "s ans anof eaf andor oricly at oo uthethel\n",
      "arate; cacfeoo ind mophemi\n",
      "ongl tuthelwumoupeoud\n",
      "andnly fopeld atd--esjothe\n",
      "1echalt, anebout moued btrwivfdorm\n",
      "pe que ser; thallo qino hime tinimow aperice iaseu-dy end\n",
      "dinn \n",
      "orhis hol asi\" eome hett ug mindim oadmtho is arl oreof wisss fucaedithe thatmuce\n",
      "sim-depeepous, tokcsa ute cheturnecs at or b. fan\n",
      "thin touphokp-ovliw \n",
      "\n",
      "diversity 1.200000:\n",
      " marreer, coti ppporyicwuof io gamimr,ay,go\n",
      ".i suarere sctsded co f\n",
      "linc hanuver-sodgo atthumd th\n",
      "ale ly  haw ge)acereol, oxv\n",
      "se? ce5bedsly [f aniwtror en axijoa, atiaveorees intewsthem\n",
      "ron\n",
      "oosis?---sricaryfermatal bnomonoce teif iasy torecvopteithi rn\"orstimiy t lte licoisly and atnby taced\n",
      "au m-, ous siaralee- )oincotqi[ar, an heucj what[ wees, andth\n",
      "mou uostill seapindeve . ouppryecefgnctudo ga\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
